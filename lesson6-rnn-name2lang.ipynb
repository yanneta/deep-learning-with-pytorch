{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifing last names with character-level RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "`wget https://github.com/hunkim/PyTorchZeroToAll/blob/master/data/names_train.csv.gz`\n",
    "\n",
    "`wget https://github.com/hunkim/PyTorchZeroToAll/blob/master/data/names_test.csv.gz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/data2/yinterian/name_dataset/names_test.csv'),\n",
       " PosixPath('/data2/yinterian/name_dataset/names_train.csv.gz'),\n",
       " PosixPath('/data2/yinterian/name_dataset/names_train.csv'),\n",
       " PosixPath('/data2/yinterian/name_dataset/names_test.csv.gz')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = Path(\"/data2/yinterian/name_dataset/\")\n",
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Adsit\",\"Czech\"\r",
      "\r\n",
      "\"Ajdrna\",\"Czech\"\r",
      "\r\n",
      "\"Antonowitsch\",\"Czech\"\r",
      "\r\n",
      "\"Antonowitz\",\"Czech\"\r",
      "\r\n",
      "\"Ballalatak\",\"Czech\"\r",
      "\r\n",
      "\"Ballaltick\",\"Czech\"\r",
      "\r\n",
      "\"Bastl\",\"Czech\"\r",
      "\r\n",
      "\"Baroch\",\"Czech\"\r",
      "\r\n",
      "\"Betlach\",\"Czech\"\r",
      "\r\n",
      "\"Biganska\",\"Czech\"\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! head /data2/yinterian/name_dataset/names_train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH/\"names_train.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ', \"'\", ',', 'A', 'B', 'C', 'D', 'E', 'F', 'G']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting a vocabulary of characters\n",
    "letters = [list(l) for l in df[0].values]\n",
    "vocab = sorted(list(set(np.concatenate(np.array(letters)))))\n",
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab2id = {key:i for i, key in enumerate(vocab)}\n",
    "vocab2id[\" \"] # I am going to use 0 to pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Arabic': 0,\n",
       " 'Chinese': 1,\n",
       " 'Czech': 2,\n",
       " 'Dutch': 3,\n",
       " 'English': 4,\n",
       " 'French': 5,\n",
       " 'German': 6,\n",
       " 'Greek': 7,\n",
       " 'Irish': 8,\n",
       " 'Italian': 9,\n",
       " 'Japanese': 10,\n",
       " 'Korean': 11,\n",
       " 'Polish': 12,\n",
       " 'Portuguese': 13,\n",
       " 'Russian': 14,\n",
       " 'Scottish': 15,\n",
       " 'Spanish': 16,\n",
       " 'Vietnamese': 17}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = sorted(df[1].unique())\n",
    "label2id = {key:i for i, key in enumerate(labels)}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(x, seq_len=15, vocab2id=vocab2id):\n",
    "    x = list(x)\n",
    "    x = np.array([vocab2id[k] for k in x])\n",
    "    z = np.zeros(seq_len, dtype=np.int32)\n",
    "    n = min(seq_len, x.shape[0])\n",
    "    z[seq_len - n:] = x[0:n]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 29, 29, 30, 30, 30],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pad_seq(\"aabbb\")\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "def seq2matrix(x, vocab_len=55):\n",
    "    z = np.zeros((x.shape[0], vocab_len))\n",
    "    z[np.arange(len(x)), x] = 1\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameDataset(Dataset):\n",
    "    def __init__(self, path, vocab2id, label2id, seq_len=15, vocab_len=55):\n",
    "        self.df = pd.read_csv(path, header=None)\n",
    "        self.label2id = label2id\n",
    "        self.vocab2id = vocab2id\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_len = vocab_len \n",
    "        self.x = df[0].values\n",
    "        self.y = [self.label2id[l] for l in df[1].values]\n",
    "        self.vocab2id = vocab2id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = pad_seq(self.x[idx], self.seq_len, self.vocab2id)\n",
    "        x = seq2matrix(x, self.vocab_len)\n",
    "        return x, self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = NameDataset(PATH/\"names_train.csv\", vocab2id, label2id)\n",
    "val = NameDataset(PATH/\"names_test.csv\", vocab2id, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2000\n",
    "n = len(test)\n",
    "train_dl = DataLoader(train, batch_size=batch_size)\n",
    "val_dl = DataLoader(test, batch_size=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13374, 13374)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 55) 2\n"
     ]
    }
   ],
   "source": [
    "x,y = train[0]\n",
    "print(x.shape,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CharRNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.linear_i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.linear_h2o = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        combined = torch.cat((x, hidden), 1)\n",
    "        hidden = torch.tanh(self.linear_i2h(combined))\n",
    "        output = self.linear_h2o(hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, bash_size):\n",
    "        return torch.zeros(bash_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 55\n",
    "hidden_size = 100\n",
    "n_classes = 18\n",
    "model = CharRNN(vocab_size, hidden_size, n_classes).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2000, 15, 55]), torch.Size([2000]))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = x.shape[0]\n",
    "h = model.initHidden(batch).cuda()\n",
    "x = x.cuda().float()\n",
    "y = y.cuda().long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000, 155])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((x[:,0], h), 1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ei in range(x.shape[1]):\n",
    "    x_t, h = model(x[:,ei], h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8333206176757812"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that just the last x_t is used in the loss\n",
    "loss = F.cross_entropy(x_t, y)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 55\n",
    "hidden_size = 100\n",
    "n_classes = 18\n",
    "model = CharRNN(vocab_size, hidden_size, n_classes).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, lr = 0.01, wd = 0.00001):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optim = torch.optim.Adam(parameters, lr=lr, weight_decay=wd)\n",
    "    return optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optim, train_dl):\n",
    "    model.train()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    for x, y in train_dl:\n",
    "        batch = x.shape[0]\n",
    "        h = model.initHidden(batch).cuda()\n",
    "        loss = 0\n",
    "        x = x.cuda().float()\n",
    "        y = y.cuda().long()\n",
    "        \n",
    "        for t in range(x.shape[1]):\n",
    "            out, h = model(x[:,t], h)\n",
    "        \n",
    "        loss = F.cross_entropy(out, y)\n",
    "        optim.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optim.step()\n",
    "        total += batch\n",
    "        sum_loss += batch*(loss.item())\n",
    "    return sum_loss/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_metric(model, val_dl):\n",
    "    model.eval()\n",
    "    x, y = next(iter(val_dl))\n",
    "    x = x.cuda().float()\n",
    "    y = y.cuda().long()\n",
    "    N = x.shape[0]\n",
    "    h = model.initHidden(N).cuda()\n",
    "    for t in range(x.shape[1]):\n",
    "        out, h = model(x[:,t], h)\n",
    "    loss = F.cross_entropy(out, y)\n",
    "    _, pred = torch.max(out, 1)\n",
    "    acc = pred.eq(y).sum().float()/N\n",
    "    return loss.item(), acc.item()\n",
    "    print(\"test loss %.3f and accuracy %.3f\" % (loss.item(), acc.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 55\n",
    "hidden_size = 80\n",
    "n_classes = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, lr, train_dl, val_dl, epochs=20):\n",
    "    optim = get_optimizer(model, lr =lr, wd = 0.0)\n",
    "    for i in range(epochs):\n",
    "        loss = train(model, optim, train_dl)\n",
    "        val_loss, val_acc = val_metric(model, val_dl)\n",
    "        if i%5 == 1: print(\"train loss %.3f val loss %.3f and val accuracy %.3f\" % (loss, val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharRNN(vocab_size, hidden_size, n_classes).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 2.204 valid loss 1.791 and accuracy 0.469\n",
      "train loss 1.771 valid loss 1.598 and accuracy 0.493\n",
      "train loss 1.446 valid loss 1.348 and accuracy 0.578\n",
      "train loss 1.257 valid loss 1.184 and accuracy 0.645\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, lr=0.01, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.219 valid loss 1.194 and accuracy 0.638\n",
      "train loss 1.147 valid loss 1.132 and accuracy 0.665\n",
      "train loss 1.101 valid loss 1.082 and accuracy 0.675\n",
      "train loss 1.067 valid loss 1.049 and accuracy 0.685\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, lr=0.001, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.037 valid loss 1.036 and accuracy 0.688\n",
      "train loss 1.022 valid loss 1.008 and accuracy 0.699\n",
      "train loss 1.001 valid loss 0.985 and accuracy 0.704\n",
      "train loss 0.984 valid loss 0.967 and accuracy 0.708\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, lr=0.001, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with character embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameDatasetEmb(Dataset):\n",
    "    def __init__(self, path, vocab2id, label2id, seq_len=15, vocab_len=55):\n",
    "        self.df = pd.read_csv(path, header=None)\n",
    "        self.label2id = label2id\n",
    "        self.vocab2id = vocab2id\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_len = vocab_len \n",
    "        self.x = df[0].values\n",
    "        self.y = [self.label2id[l] for l in df[1].values]\n",
    "        self.vocab2id = vocab2id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = pad_seq(self.x[idx], self.seq_len, self.vocab2id)\n",
    "        return x, self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2 = NameDatasetEmb(PATH/\"names_train.csv\", vocab2id, label2id)\n",
    "val_2 = NameDatasetEmb(PATH/\"names_test.csv\", vocab2id, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2000\n",
    "n = len(test)\n",
    "train_dl_2 = DataLoader(train_2, batch_size=batch_size)\n",
    "val_dl_2 = DataLoader(val_2, batch_size=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  3, 32, 47, 37, 48],\n",
       "       dtype=int32), 2)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharEmbRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, output_size):\n",
    "        super(CharEmbRNN, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.linear_i2h = nn.Linear(emb_size + hidden_size, hidden_size)\n",
    "        self.linear_h2o = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = x.long()\n",
    "        x = self.emb(x)\n",
    "        combined = torch.cat((x, hidden), 1)\n",
    "        hidden = F.tanh(self.linear_i2h(combined))\n",
    "        output = self.linear_h2o(hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, bash_size):\n",
    "        return torch.zeros(bash_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, epochs, lr=0.01, wd=0.0):\n",
    "    optim = get_optimizer(model, lr = lr, wd = wd)\n",
    "    for i in range(epochs):\n",
    "        loss = train(model, optim, train_2_dl)\n",
    "        if i%5 == 1: print(\"train loss %.3f\" % loss)\n",
    "    predict(model, test_2_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 55\n",
    "emb_size = 30\n",
    "hidden_size = 80\n",
    "n_classes = 18\n",
    "model = CharEmbRNN(vocab_size, emb_size, hidden_size, n_classes).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.761 val loss 1.898 and val accuracy 0.474\n",
      "train loss 1.294 val loss 1.201 and val accuracy 0.622\n",
      "train loss 1.096 val loss 1.023 and val accuracy 0.679\n",
      "train loss 0.988 val loss 0.913 and val accuracy 0.718\n",
      "train loss 0.886 val loss 0.832 and val accuracy 0.749\n",
      "train loss 0.825 val loss 0.760 and val accuracy 0.772\n",
      "train loss 0.851 val loss 0.752 and val accuracy 0.770\n",
      "train loss 0.743 val loss 0.678 and val accuracy 0.792\n",
      "train loss 0.673 val loss 0.624 and val accuracy 0.811\n",
      "train loss 0.641 val loss 0.586 and val accuracy 0.821\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, 0.01, train_dl_2, val_dl_2, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.752 val loss 0.731 and val accuracy 0.772\n",
      "train loss 0.614 val loss 0.568 and val accuracy 0.823\n",
      "train loss 0.553 val loss 0.508 and val accuracy 0.844\n",
      "train loss 0.508 val loss 0.464 and val accuracy 0.856\n",
      "train loss 0.469 val loss 0.424 and val accuracy 0.870\n",
      "train loss 0.440 val loss 0.391 and val accuracy 0.878\n",
      "train loss 0.421 val loss 0.381 and val accuracy 0.882\n",
      "train loss 0.369 val loss 0.332 and val accuracy 0.898\n",
      "train loss 0.359 val loss 0.310 and val accuracy 0.907\n",
      "train loss 0.343 val loss 0.299 and val accuracy 0.908\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, 0.01, train_dl_2, val_dl_2, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.512 val loss 0.463 and val accuracy 0.846\n",
      "train loss 0.308 val loss 0.277 and val accuracy 0.916\n",
      "train loss 0.276 val loss 0.255 and val accuracy 0.922\n",
      "train loss 0.251 val loss 0.226 and val accuracy 0.932\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, 0.01, train_dl_2, val_dl_2, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.448 val loss 0.406 and val accuracy 0.861\n",
      "train loss 0.234 val loss 0.215 and val accuracy 0.937\n",
      "train loss 0.201 val loss 0.182 and val accuracy 0.947\n",
      "train loss 0.241 val loss 0.203 and val accuracy 0.938\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, 0.01, train_dl_2, val_dl_2, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.178 val loss 0.172 and val accuracy 0.950\n",
      "train loss 0.163 val loss 0.160 and val accuracy 0.955\n",
      "train loss 0.156 val loss 0.154 and val accuracy 0.957\n",
      "train loss 0.152 val loss 0.150 and val accuracy 0.958\n",
      "train loss 0.148 val loss 0.146 and val accuracy 0.960\n",
      "train loss 0.145 val loss 0.143 and val accuracy 0.961\n",
      "train loss 0.142 val loss 0.140 and val accuracy 0.962\n",
      "train loss 0.139 val loss 0.137 and val accuracy 0.963\n",
      "train loss 0.136 val loss 0.134 and val accuracy 0.964\n",
      "train loss 0.134 val loss 0.132 and val accuracy 0.965\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, 0.001, train_dl_2, val_dl_2, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "* Change the first model to learn a character language model that generates last names.\n",
    "* Use one cycle training on this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "This notebook is a modified version of this tutorial\n",
    "http://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html. Here I implement vanilla RNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
